{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Statistical Based Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "    As AI With the i The develo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Fatima\n",
      "[nltk_data]     Azfar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Fatima\n",
      "[nltk_data]     Azfar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def frequency_table(text):\n",
    "    \"\"\"\n",
    "    Function to create a frequency table of words in the text, ignoring stopwords.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    freq_table = {}\n",
    "    \n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "    return freq_table\n",
    "\n",
    "def score_sentences(sentences, freq_table):\n",
    "    \"\"\"\n",
    "    Function to score sentences based on the frequency of the words that appear in them.\n",
    "    \"\"\"\n",
    "    sentence_weight = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount = 0\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word.lower() in freq_table:\n",
    "                sentence_wordcount += 1\n",
    "                if sentence[:10] in sentence_weight:\n",
    "                    sentence_weight[sentence[:10]] += freq_table[word.lower()]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:10]] = freq_table[word.lower()]\n",
    "                    \n",
    "        sentence_weight[sentence[:10]] = sentence_weight.get(sentence[:10], 0) / sentence_wordcount\n",
    "        \n",
    "    return sentence_weight\n",
    "\n",
    "def summarize_text(text, max_summary_length=3):\n",
    "    \"\"\"\n",
    "    Function to summarize the text.\n",
    "    \"\"\"\n",
    "    # Create a frequency table for words\n",
    "    freq_table = frequency_table(text)\n",
    "    \n",
    "    # Tokenize the sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Score the sentences\n",
    "    sentence_scores = score_sentences(sentences, freq_table)\n",
    "    \n",
    "    # Sort the sentences in descending order of importance\n",
    "    sorted_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    summary = ' '.join([item[0] for item in sorted_sentences[:max_summary_length]])\n",
    "    \n",
    "    return summary\n",
    "\n",
    "text = \"\"\"\n",
    "    As AI continues to evolve, the implications for the tech industry are profound. The development of advanced AI systems has the potential to reshape the landscape of technology and business, offering new opportunities and challenges alike. With the increasing integration of AI into various sectors, companies must adapt to stay competitive and innovative.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Pre-Trained Text Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fae54d5c85e4b6699fc62e21f3ee49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatima Azfar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Fatima Azfar\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354af01815564cc980bb9aadca3f81c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740940e98a404d5dbd4e537943124854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:02<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9757ee0c4c442aa871ca9bce98ab838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d661a5ca037f498fa8932363061b0644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:  The development of advanced AI systems has the potential to reshape the landscape of technology and business . With the increasing integration of AI into various sectors, companies must adapt to stay competitive . The ethical and societal impacts of AI deployments must be carefully considered .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Function to summarize text using a pre-trained model from Hugging Face's transformers library.\n",
    "    \"\"\"\n",
    "    # Load the summarization pipeline\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)\n",
    "    \n",
    "    # Extract and return the summary text\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "text = \"\"\"\n",
    "    As AI continues to evolve, the implications for the tech industry are profound. The development of advanced AI systems has the potential to reshape the landscape of technology and business, offering new opportunities and challenges alike. With the increasing integration of AI into various sectors, companies must adapt to stay competitive and innovative. Moreover, the ethical and societal impacts of AI deployments must be carefully considered to ensure they benefit society as a whole.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_text(text)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence-to-Sequence (Seq2Seq) model with LSTM units\n",
    "\n",
    "Here's a breakdown of the components and the architecture:\n",
    "\n",
    "1. **Encoder-Decoder Architecture**: This model consists of two primary components: an encoder and a decoder.\n",
    "   - **Encoder**: Takes the input sequence and processes it into a fixed-sized vector (or state), capturing the essence of the input data.\n",
    "   - **Decoder**: Takes the output from the encoder and generates the target sequence. The initial state of the decoder is set to the final state of the encoder, allowing the decoder to use the learned context.\n",
    "\n",
    "2. **LSTM Layers**: Both the encoder and decoder use Long Short-Term Memory (LSTM) layers, which are a type of recurrent neural network (RNN) suitable for sequence prediction problems. LSTM helps the model to retain long-term dependencies and handle vanishing gradient problems that can occur with standard RNNs.\n",
    "\n",
    "3. **Embedding Layer**: Both the encoder and decoder are equipped with an embedding layer that transforms the integer-encoded vocabulary into dense vectors of a fixed size. This provides a more expressive representation of words and reduces the dimensionality compared to one-hot encoding.\n",
    "\n",
    "4. **Dense Layer**: The output of the decoder's LSTM is passed through a dense (fully connected) layer with a softmax activation function to predict the probability distribution over the vocabulary for each time step in the output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Used for Training\n",
    "We are using the BBC News Summary data to train our model: https://www.kaggle.com/datasets/pariza/bbc-news-summary?select=BBC+News+Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts:  2225\n",
      "Number of summaries:  2225\n",
      "Example text:  Ad sales boost Time Warner profit\n",
      "\n",
      "Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\n",
      "\n",
      "The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\n",
      "\n",
      "Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\n",
      "\n",
      "Time Warner's fourth quarter profits were slightly better than analysts' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\n",
      "\n",
      "TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n",
      "Example summary:  TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.Time Warner's fourth quarter profits were slightly better than analysts' expectations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chardet\n",
    "\n",
    "def read_files(directory):\n",
    "    files_content = []\n",
    "    for filename in sorted(os.listdir(directory)):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            # Detect encoding\n",
    "            with open(filepath, 'rb') as file:  # Open file in binary mode\n",
    "                raw_data = file.read()\n",
    "                encoding = chardet.detect(raw_data)['encoding']\n",
    "            \n",
    "            # Read file with detected encoding\n",
    "            with open(filepath, 'r', encoding=encoding) as file:\n",
    "                files_content.append(file.read().strip())\n",
    "    return files_content\n",
    "\n",
    "def load_data(main_directory):\n",
    "    \"\"\"\n",
    "    Function to load news articles and their summaries from given directory structure.\n",
    "    \"\"\"\n",
    "    categories = ['business', 'entertainment', 'politics', 'sport', 'tech']  # List of categories\n",
    "    texts = []\n",
    "    summaries = []\n",
    "\n",
    "    # Paths for articles and summaries\n",
    "    articles_path = os.path.join(main_directory, 'News Articles')\n",
    "    summaries_path = os.path.join(main_directory, 'Summaries')\n",
    "\n",
    "    for category in categories:\n",
    "        # Full path to category folder for articles and summaries\n",
    "        category_articles_path = os.path.join(articles_path, category)\n",
    "        category_summaries_path = os.path.join(summaries_path, category)\n",
    "\n",
    "        # Read all articles and summaries from category folder\n",
    "        category_articles = read_files(category_articles_path)\n",
    "        category_summaries = read_files(category_summaries_path)\n",
    "\n",
    "        # Extend the main lists\n",
    "        texts.extend(category_articles)\n",
    "        summaries.extend(category_summaries)\n",
    "\n",
    "    return texts, summaries\n",
    "\n",
    "main_directory = 'BBC News Summary'\n",
    "\n",
    "texts, summaries = load_data(main_directory)\n",
    "print(\"Number of texts: \", len(texts))\n",
    "print(\"Number of summaries: \", len(summaries))\n",
    "print(\"Example text: \", texts[0])\n",
    "print(\"Example summary: \", summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts + summaries)\n",
    "\n",
    "# Convert texts to sequences\n",
    "text_seq = tokenizer.texts_to_sequences(texts)\n",
    "summary_seq = tokenizer.texts_to_sequences(summaries)\n",
    "\n",
    "# Pad sequences\n",
    "text_seq = pad_sequences(text_seq, maxlen=50)\n",
    "summary_seq = pad_sequences(summary_seq, maxlen=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "lstm_units = 128\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "140/140 [==============================] - 52s 267ms/step - loss: 8.1660\n",
      "Epoch 2/50\n",
      "140/140 [==============================] - 40s 283ms/step - loss: 7.2069\n",
      "Epoch 3/50\n",
      "140/140 [==============================] - 42s 299ms/step - loss: 7.0852\n",
      "Epoch 4/50\n",
      "140/140 [==============================] - 44s 314ms/step - loss: 6.9982\n",
      "Epoch 5/50\n",
      "140/140 [==============================] - 42s 303ms/step - loss: 6.9201\n",
      "Epoch 6/50\n",
      "140/140 [==============================] - 41s 290ms/step - loss: 6.8309\n",
      "Epoch 7/50\n",
      "140/140 [==============================] - 40s 289ms/step - loss: 6.7151\n",
      "Epoch 8/50\n",
      "140/140 [==============================] - 41s 290ms/step - loss: 6.5959\n",
      "Epoch 9/50\n",
      "140/140 [==============================] - 43s 307ms/step - loss: 6.4747\n",
      "Epoch 10/50\n",
      "140/140 [==============================] - 42s 300ms/step - loss: 6.3623\n",
      "Epoch 11/50\n",
      "140/140 [==============================] - 40s 286ms/step - loss: 6.2578\n",
      "Epoch 12/50\n",
      "140/140 [==============================] - 40s 284ms/step - loss: 6.1455\n",
      "Epoch 13/50\n",
      "140/140 [==============================] - 41s 292ms/step - loss: 6.0327\n",
      "Epoch 14/50\n",
      "140/140 [==============================] - 40s 287ms/step - loss: 5.9198\n",
      "Epoch 15/50\n",
      "140/140 [==============================] - 40s 286ms/step - loss: 5.8097\n",
      "Epoch 16/50\n",
      "140/140 [==============================] - 40s 288ms/step - loss: 5.7011\n",
      "Epoch 17/50\n",
      "140/140 [==============================] - 40s 286ms/step - loss: 5.5947\n",
      "Epoch 18/50\n",
      "140/140 [==============================] - 41s 293ms/step - loss: 5.4931\n",
      "Epoch 19/50\n",
      "140/140 [==============================] - 39s 280ms/step - loss: 5.3936\n",
      "Epoch 20/50\n",
      "140/140 [==============================] - 39s 281ms/step - loss: 5.2945\n",
      "Epoch 21/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 5.1938\n",
      "Epoch 22/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 5.0952\n",
      "Epoch 23/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 4.9970\n",
      "Epoch 24/50\n",
      "140/140 [==============================] - 38s 271ms/step - loss: 4.9014\n",
      "Epoch 25/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 4.8042\n",
      "Epoch 26/50\n",
      "140/140 [==============================] - 39s 275ms/step - loss: 4.7106\n",
      "Epoch 27/50\n",
      "140/140 [==============================] - 37s 268ms/step - loss: 4.6171\n",
      "Epoch 28/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 4.5214\n",
      "Epoch 29/50\n",
      "140/140 [==============================] - 38s 271ms/step - loss: 4.4278\n",
      "Epoch 30/50\n",
      "140/140 [==============================] - 37s 268ms/step - loss: 4.3375\n",
      "Epoch 31/50\n",
      "140/140 [==============================] - 37s 268ms/step - loss: 4.2477\n",
      "Epoch 32/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 4.1554\n",
      "Epoch 33/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 4.0667\n",
      "Epoch 34/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 3.9805\n",
      "Epoch 35/50\n",
      "140/140 [==============================] - 38s 274ms/step - loss: 3.8910\n",
      "Epoch 36/50\n",
      "140/140 [==============================] - 38s 272ms/step - loss: 3.8046\n",
      "Epoch 37/50\n",
      "140/140 [==============================] - 38s 270ms/step - loss: 3.7202\n",
      "Epoch 38/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 3.6356\n",
      "Epoch 39/50\n",
      "140/140 [==============================] - 38s 271ms/step - loss: 3.5530\n",
      "Epoch 40/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 3.4739\n",
      "Epoch 41/50\n",
      "140/140 [==============================] - 37s 267ms/step - loss: 3.3976\n",
      "Epoch 42/50\n",
      "140/140 [==============================] - 38s 270ms/step - loss: 3.3185\n",
      "Epoch 43/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 3.2418\n",
      "Epoch 44/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 3.1719\n",
      "Epoch 45/50\n",
      "140/140 [==============================] - 38s 268ms/step - loss: 3.0987\n",
      "Epoch 46/50\n",
      "140/140 [==============================] - 37s 267ms/step - loss: 3.0306\n",
      "Epoch 47/50\n",
      "140/140 [==============================] - 37s 266ms/step - loss: 2.9611\n",
      "Epoch 48/50\n",
      "140/140 [==============================] - 38s 269ms/step - loss: 2.8960\n",
      "Epoch 49/50\n",
      "140/140 [==============================] - 37s 268ms/step - loss: 2.8339\n",
      "Epoch 50/50\n",
      "140/140 [==============================] - 38s 270ms/step - loss: 2.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29952dbb990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare decoder input data that just contains the start token\n",
    "decoder_input_data = np.zeros_like(summary_seq)\n",
    "decoder_input_data[:, 1:] = summary_seq[:, :-1]\n",
    "decoder_input_data[:, 0] = 1  # Assuming 1 is the start token\n",
    "\n",
    "# Prepare decoder target data\n",
    "decoder_target_data = np.expand_dims(summary_seq, -1)\n",
    "\n",
    "# Training the model\n",
    "model.fit([text_seq, decoder_input_data], decoder_target_data, batch_size=16, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 683ms/step\n",
      "1/1 [==============================] - 1s 780ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      " for example was in the us and whether people's privacy\n"
     ]
    }
   ],
   "source": [
    "# Inference setup: Define encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Inference setup: Define decoder model\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Function to decode sequence\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1 with only the start character.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = 1  # Start token\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = tokenizer.index_word.get(sampled_token_index, 'unknown')  # Handle unknown tokens\n",
    "\n",
    "        # Append sampled word (or 'unknown') to the decoded sentence\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == 'end' or len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "print(decode_sequence(text_seq[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Summary:  for example was in the us and whether people's privacy\n"
     ]
    }
   ],
   "source": [
    "input_text = input(\"Enter a text to summarize: \")\n",
    "input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "input_seq = pad_sequences(input_seq, maxlen=50)\n",
    "summary = decode_sequence(input_seq)\n",
    "print(\"Summary:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
